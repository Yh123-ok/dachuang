import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
# ç§»é™¤ train_test_splitï¼Œå› ä¸º LOSO ä¸éœ€è¦éšæœºåˆ‡åˆ†
from sklearn.metrics import f1_score, accuracy_score
import numpy as np
import os
import scipy.io as sio

# ==========================================
# ğŸ› ï¸ æ¨¡å— 1: åŸºç¡€ç»„ä»¶ - æ®‹å·®åŠ¨æ€å›¾å±‚ (ä¿æŒä¸å˜)
# ==========================================
class ResDGFLayer(nn.Module):
    """
    æ ¸å¿ƒç»„ä»¶ï¼šåŠ¨æ€å›¾å·ç§¯ + æ®‹å·®è¿æ¥ + LayerNorm
    èƒ½å¤Ÿæ ¹æ®ç‰¹å¾å†…å®¹çš„ç›¸ä¼¼æ€§åŠ¨æ€æ„å»ºé‚»æ¥çŸ©é˜µã€‚
    """
    def __init__(self, d_model, num_heads=4):
        super().__init__()
        self.num_heads = num_heads
        self.log_sigmas = nn.Parameter(torch.zeros(num_heads, 1, 1))
        self.proj = nn.Linear(d_model, d_model)
        self.norm = nn.LayerNorm(d_model)
        self.activation = nn.ELU()

    def forward(self, x):
        b, n, d = x.size()
        dist_sq = torch.cdist(x, x, p=2)**2 
        sigmas = torch.exp(self.log_sigmas) 
        adjs = torch.exp(-dist_sq.unsqueeze(1) / (2 * sigmas.unsqueeze(0)**2 + 1e-6))
        adj_final = adjs.mean(dim=1) 
        out = torch.matmul(adj_final, x) 
        out = self.proj(out)            
        return self.norm(self.activation(out) + x)

# ==========================================
# ğŸ› ï¸ æ¨¡å— 2: æ·±å±‚ GNN å †å å— (ä¿æŒä¸å˜)
# ==========================================
class DeepGNNBlock(nn.Module):
    def __init__(self, d_model, layers=3, num_heads=4):
        super().__init__()
        self.layers = nn.ModuleList([
            ResDGFLayer(d_model, num_heads) for _ in range(layers)
        ])
        
    def forward(self, x):
        for layer in self.layers:
            x = layer(x)
        return x

# ==========================================
# ğŸ§  æ¨¡å— 3: Deep-Res-MT-DGF-GNN æ¨¡å‹ä¸»ä½“ (ä¿æŒä¸å˜)
# ==========================================
class Deep_MT_DGF_GNN(nn.Module):
    def __init__(self, hidden_dim=64):
        super().__init__()
        
        # --- åˆ†æ”¯ A: CNN ---
        self.cnn_net = nn.Sequential(
            nn.Conv2d(5, 16, 3, padding=1), nn.BatchNorm2d(16), nn.ELU(),
            nn.MaxPool2d(2),
            nn.Conv2d(16, 32, 3, padding=1), nn.BatchNorm2d(32), nn.ELU(),
            nn.AdaptiveAvgPool2d((2, 2)), 
            nn.Flatten() 
        )
        self.cnn_proj = nn.Linear(128, hidden_dim)

        # --- åˆ†æ”¯ B: Deep GNN ---
        self.gnn_mapping = nn.Linear(7, hidden_dim)
        self.deep_gnn = DeepGNNBlock(d_model=hidden_dim, layers=3, num_heads=4)

        # --- åˆ†æ”¯ C: MLP ---
        self.peri_net = nn.Sequential(
            nn.Linear(55, hidden_dim), 
            nn.LayerNorm(hidden_dim), 
            nn.ELU()
        )

        # --- èåˆæ¨¡å— ---
        self.fusion_layer = ResDGFLayer(hidden_dim, num_heads=8)
        
        # --- å¤šä»»åŠ¡è¾“å‡ºå¤´ ---
        combined_dim = hidden_dim * 3
        self.v_head = nn.Sequential(nn.Linear(combined_dim, 64), nn.ELU(), nn.Linear(64, 2))
        self.a_head = nn.Sequential(nn.Linear(combined_dim, 64), nn.ELU(), nn.Linear(64, 2))
        
        # --- è‡ªåŠ¨æƒé‡å‚æ•° ---
        self.log_vars = nn.Parameter(torch.zeros(2)) 

    def forward(self, maps, stats, peri):
        h_cnn = self.cnn_proj(self.cnn_net(maps)).unsqueeze(1) 
        
        gnn_in = self.gnn_mapping(stats) 
        h_gnn_nodes = self.deep_gnn(gnn_in) 
        h_gnn = h_gnn_nodes.mean(dim=1, keepdim=True) 
        
        h_peri = self.peri_net(peri).unsqueeze(1)

        combined = torch.cat([h_cnn, h_gnn, h_peri], dim=1) 
        fused = self.fusion_layer(combined)
        
        flat_feat = fused.view(fused.size(0), -1) 
        return self.v_head(flat_feat), self.a_head(flat_feat)

# ==========================================
# ğŸ’¾ æ¨¡å— 4: æ•°æ®åŠ è½½ (ä¿®æ”¹æ”¯æŒæŒ‡å®šæ–‡ä»¶åˆ—è¡¨)
# ==========================================
class DeapMultiModalDataset(Dataset):
    def __init__(self, npz_dir, raw_mat_dir, filenames=None):
        """
        ä¿®æ”¹ç‚¹ï¼šå¢åŠ äº† filenames å‚æ•°ã€‚
        å¦‚æœä¸ä¼  filenamesï¼Œé»˜è®¤è¯»å–ç›®å½•ä¸‹æ‰€æœ‰æ–‡ä»¶ï¼ˆå…¼å®¹æ—§é€»è¾‘ï¼‰ã€‚
        å¦‚æœä¼ äº† filenamesï¼Œåˆ™åªåŠ è½½åˆ—è¡¨ä¸­çš„è¢«è¯•ï¼ˆç”¨äºæ„å»º Train/Test setï¼‰ã€‚
        """
        self.npz_dir = npz_dir
        
        # é€»è¾‘ä¿®æ”¹ï¼šå¦‚æœå¤–éƒ¨ä¼ å…¥äº†æ–‡ä»¶åˆ—è¡¨ï¼Œç›´æ¥ä½¿ç”¨ï¼›å¦åˆ™æ‰«æç›®å½•
        if filenames is not None:
            self.file_list = filenames
        else:
            self.file_list = sorted([f for f in os.listdir(npz_dir) if f.endswith('.npz')])
            
        self.samples_per_trial = 15 
        
        self.v_labels = []
        self.a_labels = []
        
        # ä»…æ‰“å°ç®€ç•¥ä¿¡æ¯é¿å…åˆ·å±
        # print(f"æ­£åœ¨åŠ è½½ {len(self.file_list)} ä¸ªè¢«è¯•çš„æ•°æ®...")
        
        for f_name in self.file_list:
            subj_id = f_name[:3] 
            mat_path = os.path.join(raw_mat_dir, f"{subj_id}.mat")
            
            if not os.path.exists(mat_path):
                print(f"è­¦å‘Š: æ‰¾ä¸åˆ°å¯¹åº”çš„æ ‡ç­¾æ–‡ä»¶ {mat_path}ï¼Œè·³è¿‡è¯¥æ–‡ä»¶ã€‚")
                continue
                
            raw_labels = sio.loadmat(mat_path)['labels']
            v_binary = (raw_labels[:, 0] > 5).astype(np.int64)
            a_binary = (raw_labels[:, 1] > 5).astype(np.int64)
            
            self.v_labels.append(np.repeat(v_binary, self.samples_per_trial))
            self.a_labels.append(np.repeat(a_binary, self.samples_per_trial))
            
        if len(self.v_labels) > 0:
            self.v_labels = np.concatenate(self.v_labels)
            self.a_labels = np.concatenate(self.a_labels)
        else:
            # é˜²æ­¢ç©ºåˆ—è¡¨æŠ¥é”™
            self.v_labels = np.array([])
            self.a_labels = np.array([])
        
        # print(f"æ•°æ®åŠ è½½å®Œæˆã€‚æ ·æœ¬æ•°: {len(self.v_labels)}")

    def __len__(self):
        return len(self.v_labels)

    def __getitem__(self, idx):
        file_idx = idx // 600 
        inner_idx = idx % 600
        
        file_path = os.path.join(self.npz_dir, self.file_list[file_idx])
        
        with np.load(file_path) as data:
            maps = torch.from_numpy(data['eeg_allband_feature_map'][inner_idx]).float()
            stats = torch.from_numpy(data['eeg_en_stat'][inner_idx]).view(32, 7).float()
            peri = torch.from_numpy(data['peri_feature'][inner_idx]).float()
            
        return maps, stats, peri, self.v_labels[idx], self.a_labels[idx]

# ==========================================
# ğŸš€ æ¨¡å— 5: è®­ç»ƒå¼•æ“ (ä¿®æ”¹ä¸º LOSO æµç¨‹)
# ==========================================
def train_deep_mt_dgf_loso():
    # --- é…ç½®åŒºåŸŸ ---
    NPZ_PATH = r'D:\Users\cyz\dc\222'                 
    RAW_PATH = r'E:\BaiduNetdiskDownload\DEAP\data_preprocessed_matlab'    
    BATCH_SIZE = 64
    EPOCHS = 30
    LR = 0.0005
    DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    # 1. è·å–æ‰€æœ‰æ–‡ä»¶å
    all_files = sorted([f for f in os.listdir(NPZ_PATH) if f.endswith('.npz')])
    num_subjects = len(all_files)
    
    print(f"\nğŸš€ å¼€å§‹ LOSO (Leave-One-Subject-Out) éªŒè¯")
    print(f"æ€»è¢«è¯•æ•°: {num_subjects} | Device: {DEVICE}")
    print(f"æ¨¡å‹: Deep-Res-MT-DGF-GNN | Epochs per fold: {EPOCHS}")
    
    # å­˜å‚¨æ¯ä¸ª fold (æ¯ä¸ªè¢«è¯•) çš„ç»“æœ
    loso_results = {
        'subject': [],
        'v_acc': [], 'v_f1': [],
        'a_acc': [], 'a_f1': []
    }
    
    # --- LOSO ä¸»å¾ªç¯ ---
    for i, test_file in enumerate(all_files):
        subj_name = test_file.split('.')[0]
        print(f"\n[{i+1}/{num_subjects}] æ­£åœ¨æµ‹è¯•è¢«è¯•: {subj_name} ...")
        
        # 1. åˆ’åˆ†æ–‡ä»¶åˆ—è¡¨
        # æµ‹è¯•é›†ï¼šå½“å‰è¿™ 1 ä¸ªæ–‡ä»¶
        # è®­ç»ƒé›†ï¼šå‰©ä½™ N-1 ä¸ªæ–‡ä»¶
        train_files = [f for f in all_files if f != test_file]
        test_files = [test_file]
        
        # 2. æ„å»ºæ•°æ®é›†
        train_dataset = DeapMultiModalDataset(NPZ_PATH, RAW_PATH, filenames=train_files)
        test_dataset = DeapMultiModalDataset(NPZ_PATH, RAW_PATH, filenames=test_files)
        
        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)
        # æµ‹è¯•é›†ä¸åš shuffleï¼Œæ–¹ä¾¿è§‚å¯Ÿæ—¶åºï¼ˆè™½ç„¶è¿™é‡Œåªçœ‹æ€»ä½“æŒ‡æ ‡ï¼‰
        test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)
        
        # 3. åˆå§‹åŒ–æ¨¡å‹ (æ¯ä¸ª Fold å¿…é¡»é‡æ–°åˆå§‹åŒ–ï¼)
        model = Deep_MT_DGF_GNN(hidden_dim=64).to(DEVICE)
        optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-3)
        
        # 4. è®­ç»ƒå¾ªç¯ (é’ˆå¯¹å½“å‰ Fold)
        for epoch in range(1, EPOCHS + 1):
            model.train()
            # ç®€åŒ–è¾“å‡ºï¼Œåªåœ¨æœ€åå‡ ä¸ª epoch æˆ–ç‰¹å®šé—´éš”æ‰“å° lossï¼Œé¿å… LOSO æ—¥å¿—å¤ªé•¿
            
            for maps, stats, peri, lv, la in train_loader:
                maps, stats, peri = maps.to(DEVICE), stats.to(DEVICE), peri.to(DEVICE)
                lv, la = lv.to(DEVICE), la.to(DEVICE)
                
                out_v, out_a = model(maps, stats, peri)
                
                loss_v = F.cross_entropy(out_v, lv)
                loss_a = F.cross_entropy(out_a, la)
                
                # è‡ªåŠ¨æƒé‡ Loss
                precision_v = torch.exp(-model.log_vars[0])
                precision_a = torch.exp(-model.log_vars[1])
                combined_loss = (loss_v * precision_v + model.log_vars[0]) + \
                                (loss_a * precision_a + model.log_vars[1])
                
                optimizer.zero_grad()
                combined_loss.backward()
                optimizer.step()
        
        # 5. æµ‹è¯•å½“å‰è¢«è¯• (Validation/Test)
        model.eval()
        v_preds, a_preds, v_gt, a_gt = [], [], [], []
        
        with torch.no_grad():
            for maps, stats, peri, lv, la in test_loader:
                maps, stats, peri = maps.to(DEVICE), stats.to(DEVICE), peri.to(DEVICE)
                ov, oa = model(maps, stats, peri)
                
                v_preds.extend(torch.max(ov, 1)[1].cpu().numpy())
                a_preds.extend(torch.max(oa, 1)[1].cpu().numpy())
                v_gt.extend(lv.numpy())
                a_gt.extend(la.numpy())
        
        # 6. è®¡ç®—æŒ‡æ ‡
        curr_v_acc = accuracy_score(v_gt, v_preds)
        curr_a_acc = accuracy_score(a_gt, a_preds)
        curr_v_f1 = f1_score(v_gt, v_preds, average='macro')
        curr_a_f1 = f1_score(a_gt, a_preds, average='macro')
        
        print(f"   -> {subj_name} ç»“æœ: Val Acc={curr_v_acc:.4f}, Aro Acc={curr_a_acc:.4f}, Val F1={curr_v_f1:.4f}, Aro F1={curr_a_f1:.4f}")
        
        # è®°å½•ç»“æœ
        loso_results['subject'].append(subj_name)
        loso_results['v_acc'].append(curr_v_acc)
        loso_results['v_f1'].append(curr_v_f1)
        loso_results['a_acc'].append(curr_a_acc)
        loso_results['a_f1'].append(curr_a_f1)

    # --- æœ€ç»ˆ LOSO æŠ¥å‘Š ---
    print("\n" + "="*50)
    print("ğŸ† LOSO æœ€ç»ˆè¯„ä¼°æŠ¥å‘Š (Deep-Res-MT-DGF-GNN)")
    print("="*50)
    
    avg_v_acc = np.mean(loso_results['v_acc'])
    avg_a_acc = np.mean(loso_results['a_acc'])
    avg_v_f1 = np.mean(loso_results['v_f1'])
    avg_a_f1 = np.mean(loso_results['a_f1'])
    
    print(f"Valence â¡ï¸  Avg Acc: {avg_v_acc:.4f} | Avg F1: {avg_v_f1:.4f}")
    print(f"Arousal â¡ï¸  Avg Acc: {avg_a_acc:.4f} | Avg F1: {avg_a_f1:.4f}")
    print("-" * 50)
    
    # å¯é€‰ï¼šæ‰“å°æ¯ä¸ªè¢«è¯•çš„è¯¦ç»†è¡¨
    # print("è¯¦ç»†æ•°æ®:")
    # for i in range(num_subjects):
    #     print(f"Subj {loso_results['subject'][i]}: V_Acc={loso_results['v_acc'][i]:.3f}, A_Acc={loso_results['a_acc'][i]:.3f}")

if __name__ == "__main__":
    train_deep_mt_dgf_loso()