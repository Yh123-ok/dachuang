import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader, Subset
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, f1_score, accuracy_score
import numpy as np
import os
import scipy.io as sio

# ==========================================
# ğŸ› ï¸ æ¨¡å— 1: åŸºç¡€ç»„ä»¶ - æ®‹å·®åŠ¨æ€å›¾å±‚
# ==========================================
class ResDGFLayer(nn.Module):
    """
    æ ¸å¿ƒç»„ä»¶ï¼šåŠ¨æ€å›¾å·ç§¯ + æ®‹å·®è¿æ¥ + LayerNorm
    èƒ½å¤Ÿæ ¹æ®ç‰¹å¾å†…å®¹çš„ç›¸ä¼¼æ€§åŠ¨æ€æ„å»ºé‚»æ¥çŸ©é˜µã€‚
    """
    def __init__(self, d_model, num_heads=4):
        super().__init__()
        self.num_heads = num_heads
        # log_sigmas æ§åˆ¶é«˜æ–¯æ ¸çš„å®½åº¦ï¼Œè®¾ä¸ºå¯å­¦ä¹ å‚æ•°
        self.log_sigmas = nn.Parameter(torch.zeros(num_heads, 1, 1))
        self.proj = nn.Linear(d_model, d_model)
        self.norm = nn.LayerNorm(d_model)
        self.activation = nn.ELU()

    def forward(self, x):
        # x shape: [Batch, Nodes, Dim]
        b, n, d = x.size()
        
        # 1. è®¡ç®—èŠ‚ç‚¹é—´çš„æˆå¯¹æ¬§æ°è·ç¦»
        dist_sq = torch.cdist(x, x, p=2)**2 # [B, N, N]
        
        # 2. è®¡ç®—åŠ¨æ€é‚»æ¥çŸ©é˜µ (RBF Kernel)
        sigmas = torch.exp(self.log_sigmas) # ç¡®ä¿ sigma > 0
        # å¹¿æ’­æœºåˆ¶: [B, 1, N, N] / [H, 1, 1] -> [B, H, N, N]
        adjs = torch.exp(-dist_sq.unsqueeze(1) / (2 * sigmas.unsqueeze(0)**2 + 1e-6))
        
        # 3. å¤šå¤´å¹³å‡èšåˆ
        adj_final = adjs.mean(dim=1) # [B, N, N]
        
        # 4. å›¾å·ç§¯æ“ä½œ
        out = torch.matmul(adj_final, x) # Aggregation
        out = self.proj(out)             # Update
        
        # 5. æ®‹å·®è¿æ¥ä¸å½’ä¸€åŒ– (å…³é”®æ­¥éª¤ï¼Œé˜²æ­¢æ·±å±‚ç½‘ç»œé€€åŒ–)
        return self.norm(self.activation(out) + x)

# ==========================================
# ğŸ› ï¸ æ¨¡å— 2: æ·±å±‚ GNN å †å å— (Deep Stack)
# ==========================================
class DeepGNNBlock(nn.Module):
    """
    å †å å¤šä¸ª ResDGFLayerï¼Œå¢åŠ æ„Ÿå—é‡ï¼Œæ•æ‰é«˜é˜¶å…³ç³»ã€‚
    """
    def __init__(self, d_model, layers=3, num_heads=4):
        super().__init__()
        self.layers = nn.ModuleList([
            ResDGFLayer(d_model, num_heads) for _ in range(layers)
        ])
        
    def forward(self, x):
        for layer in self.layers:
            x = layer(x)
        return x

# ==========================================
# ğŸ§  æ¨¡å— 3: Deep-Res-MT-DGF-GNN æ¨¡å‹ä¸»ä½“
# ==========================================
class Deep_MT_DGF_GNN(nn.Module):
    def __init__(self, hidden_dim=64):
        super().__init__()
        
        # --- åˆ†æ”¯ A: CNN (å¤„ç†è„‘ç”µåœ°å½¢å›¾ 5x32x32) ---
        self.cnn_net = nn.Sequential(
            nn.Conv2d(5, 16, 3, padding=1), nn.BatchNorm2d(16), nn.ELU(),
            nn.MaxPool2d(2),
            nn.Conv2d(16, 32, 3, padding=1), nn.BatchNorm2d(32), nn.ELU(),
            nn.AdaptiveAvgPool2d((2, 2)), 
            nn.Flatten() # 32*2*2 = 128
        )
        self.cnn_proj = nn.Linear(128, hidden_dim)

        # --- åˆ†æ”¯ B: Deep GNN (å¤„ç†ç”µæç»Ÿè®¡ç‰¹å¾ 32x7) ---
        self.gnn_mapping = nn.Linear(7, hidden_dim)
        # ğŸŒŸ ä½¿ç”¨ 3 å±‚æ·±åº¦çš„ GNN æå– 32 ä¸ªç”µæé—´çš„å¤æ‚ç©ºé—´å…³ç³»
        self.deep_gnn = DeepGNNBlock(d_model=hidden_dim, layers=3, num_heads=4)

        # --- åˆ†æ”¯ C: MLP (å¤„ç†å¤–å‘¨ç”Ÿç†ç‰¹å¾ 55ç»´) ---
        self.peri_net = nn.Sequential(
            nn.Linear(55, hidden_dim), 
            nn.LayerNorm(hidden_dim), 
            nn.ELU()
        )

        # --- èåˆæ¨¡å—: è·¨æ¨¡æ€åŠ¨æ€å›¾ ---
        # å°† CNN, GNN, Peri ä¸‰ä¸ªç‰¹å¾å‘é‡çœ‹ä½œå›¾ä¸­çš„ 3 ä¸ªèŠ‚ç‚¹è¿›è¡Œèåˆ
        self.fusion_layer = ResDGFLayer(hidden_dim, num_heads=8)
        
        # --- å¤šä»»åŠ¡è¾“å‡ºå¤´ (MTL Heads) ---
        # å…±äº«ç‰¹å¾ -> ç‹¬ç«‹é¢„æµ‹
        combined_dim = hidden_dim * 3
        self.v_head = nn.Sequential(nn.Linear(combined_dim, 64), nn.ELU(), nn.Linear(64, 2))
        self.a_head = nn.Sequential(nn.Linear(combined_dim, 64), nn.ELU(), nn.Linear(64, 2))
        
        # --- è‡ªåŠ¨æƒé‡å‚æ•° (Uncertainty Weights) ---
        # log_vars = log(sigma^2)ï¼Œç”¨äºå¹³è¡¡å¤šä»»åŠ¡ Loss
        self.log_vars = nn.Parameter(torch.zeros(2)) 

    def forward(self, maps, stats, peri):
        # 1. ç‰¹å¾æå–
        # CNN Branch
        h_cnn = self.cnn_proj(self.cnn_net(maps)).unsqueeze(1) # [B, 1, H]
        
        # Deep GNN Branch
        gnn_in = self.gnn_mapping(stats) 
        h_gnn_nodes = self.deep_gnn(gnn_in) # [B, 32, H] (ç»è¿‡3å±‚äº¤äº’)
        h_gnn = h_gnn_nodes.mean(dim=1, keepdim=True) # å…¨å±€èšåˆ [B, 1, H]
        
        # Peri Branch
        h_peri = self.peri_net(peri).unsqueeze(1) # [B, 1, H]

        # 2. åŠ¨æ€èåˆ (Heterogeneous Graph)
        # æ‹¼æ¥ä¸‰ä¸ªæ¨¡æ€èŠ‚ç‚¹: [B, 3, H]
        combined = torch.cat([h_cnn, h_gnn, h_peri], dim=1) 
        # å­¦ä¹ æ¨¡æ€é—´çš„æ³¨æ„åŠ›
        fused = self.fusion_layer(combined)
        
        # 3. å±•å¹³å¹¶åˆ†ç±»
        flat_feat = fused.view(fused.size(0), -1) # [B, 3*H]
        return self.v_head(flat_feat), self.a_head(flat_feat)

# ==========================================
# ğŸ’¾ æ¨¡å— 4: æ•°æ®åŠ è½½ä¸æ ‡ç­¾å¯¹é½
# ==========================================
class DeapMultiModalDataset(Dataset):
    def __init__(self, npz_dir, raw_mat_dir):
        self.npz_dir = npz_dir
        self.file_list = sorted([f for f in os.listdir(npz_dir) if f.endswith('.npz')])
        self.samples_per_trial = 15 # 4såˆ‡ç‰‡
        
        self.v_labels = []
        self.a_labels = []
        
        print(f"æ­£åœ¨åŠ è½½ {len(self.file_list)} ä¸ªè¢«è¯•çš„æ•°æ®...")
        
        for f_name in self.file_list:
            # æ–‡ä»¶åè§£æ: å‡è®¾æ ¼å¼ä¸º s01_features.npz æˆ– s01.npz
            subj_id = f_name[:3] # å–å‰ä¸‰ä¸ªå­—ç¬¦ï¼Œå¦‚ 's01'
            mat_path = os.path.join(raw_mat_dir, f"{subj_id}.mat")
            
            if not os.path.exists(mat_path):
                print(f"è­¦å‘Š: æ‰¾ä¸åˆ°å¯¹åº”çš„æ ‡ç­¾æ–‡ä»¶ {mat_path}ï¼Œè·³è¿‡è¯¥æ–‡ä»¶ã€‚")
                continue
                
            # åŠ è½½åŸå§‹ Label: [40, 4] -> Valence(0), Arousal(1)
            raw_labels = sio.loadmat(mat_path)['labels']
            
            # æå– V å’Œ A å¹¶äºŒå€¼åŒ– (é˜ˆå€¼ 5)
            v_binary = (raw_labels[:, 0] > 5).astype(np.int64)
            a_binary = (raw_labels[:, 1] > 5).astype(np.int64)
            
            # æ‰©å±•æ ‡ç­¾: Trialçº§ -> Sampleçº§ (40 -> 600)
            self.v_labels.append(np.repeat(v_binary, self.samples_per_trial))
            self.a_labels.append(np.repeat(a_binary, self.samples_per_trial))
            
        self.v_labels = np.concatenate(self.v_labels)
        self.a_labels = np.concatenate(self.a_labels)
        
        print(f"æ•°æ®åŠ è½½å®Œæˆã€‚æ€»æ ·æœ¬æ•°: {len(self.v_labels)}")

    def __len__(self):
        return len(self.v_labels)

    def __getitem__(self, idx):
        # è®¡ç®—è¯¥æ ·æœ¬å±äºå“ªä¸ªæ–‡ä»¶
        file_idx = idx // 600 
        inner_idx = idx % 600
        
        file_path = os.path.join(self.npz_dir, self.file_list[file_idx])
        
        # åŠ¨æ€è¯»å–ä»¥èŠ‚çœå†…å­˜
        with np.load(file_path) as data:
            # åœ°å½¢å›¾ [5, 32, 32]
            maps = torch.from_numpy(data['eeg_allband_feature_map'][inner_idx]).float()
            # ç»Ÿè®¡ç‰¹å¾ [32, 7] (éœ€ reshape)
            stats = torch.from_numpy(data['eeg_en_stat'][inner_idx]).view(32, 7).float()
            # å¤–å‘¨ç‰¹å¾ [55]
            peri = torch.from_numpy(data['peri_feature'][inner_idx]).float()
            
        return maps, stats, peri, self.v_labels[idx], self.a_labels[idx]

# ==========================================
# ğŸš€ æ¨¡å— 5: è®­ç»ƒå¼•æ“ä¸è¯„ä¼°
# ==========================================
def train_deep_mt_dgf():
    # --- é…ç½®åŒºåŸŸ (è¯·ä¿®æ”¹è¿™é‡Œ) ---
    NPZ_PATH = r'D:\Users\cyz\dc\222'                 # ç‰¹å¾æ–‡ä»¶å¤¹è·¯å¾„
    RAW_PATH = r'E:\BaiduNetdiskDownload\DEAP\data_preprocessed_matlab'    # åŸå§‹ .mat æ–‡ä»¶å¤¹è·¯å¾„
    BATCH_SIZE = 64
    EPOCHS = 30
    LR = 0.0005
    DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    # 1. å‡†å¤‡æ•°æ®
    dataset = DeapMultiModalDataset(NPZ_PATH, RAW_PATH)
    # 80% è®­ç»ƒ, 20% éªŒè¯
    train_idx, val_idx = train_test_split(range(len(dataset)), test_size=0.2, random_state=42)
    
    train_loader = DataLoader(Subset(dataset, train_idx), batch_size=BATCH_SIZE, shuffle=True, num_workers=0)
    val_loader = DataLoader(Subset(dataset, val_idx), batch_size=BATCH_SIZE, shuffle=False, num_workers=0)
    
    # 2. åˆå§‹åŒ–æ¨¡å‹
    model = Deep_MT_DGF_GNN(hidden_dim=64).to(DEVICE)
    # æ³¨æ„: ä¼˜åŒ–å™¨éœ€è¦åŒæ—¶æ›´æ–°æ¨¡å‹å‚æ•°å’Œè‡ªåŠ¨æƒé‡å‚æ•°
    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-3)
    
    print(f"\n å¼€å§‹è®­ç»ƒ Deep-Res-MT-DGF-GNN (Device: {DEVICE})")
    print(f" GNN æ·±åº¦: 3å±‚ |  ä»»åŠ¡: Valence & Arousal åŒæ—¶ä¼˜åŒ–")
    
    best_avg_f1 = 0
    
    for epoch in range(1, EPOCHS + 1):
        model.train()
        total_loss = 0
        
        for maps, stats, peri, lv, la in train_loader:
            maps, stats, peri = maps.to(DEVICE), stats.to(DEVICE), peri.to(DEVICE)
            lv, la = lv.to(DEVICE), la.to(DEVICE)
            
            # å‰å‘ä¼ æ’­
            out_v, out_a = model(maps, stats, peri)
            
            # è®¡ç®—ç‹¬ç«‹ Loss
            loss_v = F.cross_entropy(out_v, lv)
            loss_a = F.cross_entropy(out_a, la)
            
            # --- è‡ªåŠ¨æƒé‡ Loss è®¡ç®— (Kendall et al.) ---
            # Loss = L * exp(-log_var) + log_var
            precision_v = torch.exp(-model.log_vars[0])
            precision_a = torch.exp(-model.log_vars[1])
            combined_loss = (loss_v * precision_v + model.log_vars[0]) + \
                            (loss_a * precision_a + model.log_vars[1])
            
            optimizer.zero_grad()
            combined_loss.backward()
            optimizer.step()
            total_loss += combined_loss.item()
            
        # --- éªŒè¯é˜¶æ®µ ---
        model.eval()
        v_preds, a_preds, v_gt, a_gt = [], [], [], []
        
        with torch.no_grad():
            for maps, stats, peri, lv, la in val_loader:
                maps, stats, peri = maps.to(DEVICE), stats.to(DEVICE), peri.to(DEVICE)
                ov, oa = model(maps, stats, peri)
                
                v_preds.extend(torch.max(ov, 1)[1].cpu().numpy())
                a_preds.extend(torch.max(oa, 1)[1].cpu().numpy())
                v_gt.extend(lv.numpy())
                a_gt.extend(la.numpy())
        
        # æŒ‡æ ‡è®¡ç®—
        f1_v = f1_score(v_gt, v_preds, average='macro')
        f1_a = f1_score(a_gt, a_preds, average='macro')
        avg_f1 = (f1_v + f1_a) / 2
        
        # è·å–å½“å‰ä»»åŠ¡æƒé‡ (ç”¨äºè§‚å¯Ÿæ¨¡å‹ä¾§é‡)
        w_v = torch.exp(-model.log_vars[0]).item()
        w_a = torch.exp(-model.log_vars[1]).item()
        
        print(f"Epoch {epoch:02d} | Loss: {total_loss/len(train_loader):.4f} | "
              f"Val F1 -> V: {f1_v:.4f}, A: {f1_a:.4f} (Avg: {avg_f1:.4f}) | "
              f"Weights -> V: {w_v:.2f}, A: {w_a:.2f}")
        
        if avg_f1 > best_avg_f1:
            best_avg_f1 = avg_f1
            torch.save(model.state_dict(), "best_deep_mt_model.pth")
            
    # --- æœ€ç»ˆæŠ¥å‘Š ---
    print("\nğŸ† --- è®­ç»ƒç»“æŸï¼Œæœ€ä½³æ¨¡å‹æ€§èƒ½æŠ¥å‘Š ---")
    model.load_state_dict(torch.load("best_deep_mt_model.pth"))
    model.eval()
    # è¿™é‡Œçœç•¥å†æ¬¡è·‘ä¸€ééªŒè¯é›†çš„ä»£ç ï¼Œç›´æ¥ä½¿ç”¨æœ€åä¸€æ¬¡ç»“æœæˆ–é‡æ–°åŠ è½½è¿›è¡Œè¯¦ç»†æ‰“å°
    print(f"Best Average F1: {best_avg_f1:.4f}")
    # ä½ å¯ä»¥åœ¨è¿™é‡ŒåŠ å› classification_report æ‰“å°è¯¦ç»†åˆ†ç±»è¡¨

if __name__ == "__main__":
    train_deep_mt_dgf()