#!/usr/bin/env python
# -*- coding: utf-8 -*-

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from torch.utils.data import DataLoader, Dataset
import torchvision.transforms as transforms
import torchvision.models as models
import torchvision
from PIL import Image
import os
import warnings
import json
import time
from datetime import datetime
import pandas as pd
import cv2  # ç”¨äºè§†é¢‘å¤„ç†

warnings.filterwarnings('ignore')

# ==================== ç”¨æˆ·é…ç½®åŒºåŸŸï¼ˆåªéœ€è¦ä¿®æ”¹è¿™é‡Œï¼ï¼‰====================

# 1. æ¨¡å‹è·¯å¾„ï¼ˆå¿…é¡»ä¿®æ”¹ï¼‰
MODEL_PATH = '/data/coding/face2nodes_best.pth'  # æ”¹ä¸ºæ‚¨çš„æ¨¡å‹è·¯å¾„

# 2. è§†é¢‘è·¯å¾„ï¼ˆå¿…é¡»ä¿®æ”¹ï¼‰
VIDEO_PATH = '/data/coding/071309_w_21-PA4-076.mp4'       # æ”¹ä¸ºæ‚¨çš„è§†é¢‘è·¯å¾„

# 3. è¾“å‡ºç›®å½•
OUTPUT_DIR = '/data/coding/video_features_output'           # ç‰¹å¾è¾“å‡ºç›®å½•

# 4. å¤„ç†å‚æ•°
SAMPLE_RATE = 5                                   # é‡‡æ ·ç‡ï¼ˆæ¯5å¸§å¤„ç†ä¸€å¸§ï¼‰
OUTPUT_DIM = 512                                   # è¾“å‡ºç‰¹å¾ç»´åº¦

# ======================================================================

def knn(x: torch.Tensor, k: int, dilation: int = 1) -> torch.Tensor: 
    """
    è®¡ç®—è†¨èƒ€kæœ€è¿‘é‚»
    Args:
        x: èŠ‚ç‚¹ç‰¹å¾ (B, N, D)
        k: é‚»å±…æ•°
        dilation: è†¨èƒ€ç‡
    Returns:
        idx: é‚»å±…ç´¢å¼• (B, N, k)
    """
    B, N, D = x.shape
    device = x.device
    k_total = k * dilation
    xx = torch.sum(x**2, dim=2, keepdim=True)
    xy = torch.matmul(x, x.transpose(2, 1))
    pairwise_distance = xx + xx.transpose(2, 1) - 2 * xy
    idx = pairwise_distance.topk(k=k_total+1, dim=-1, largest=False)[1][:, :, 1:]
    if dilation > 1:
        idx = idx[:, :, ::dilation][:, :, :k]
    else:
        idx = idx[:, :, :k]
    return idx

# ==================== MSFÂ²PEæ¨¡å— ====================
class MultiScalePatchEmbedding(nn.Module):
    def __init__(self, in_channels=3, embed_dim=256, patch_size=1, pretrained=True):
        super().__init__()
        self.embed_dim = embed_dim
        self.patch_size = patch_size
        
        resnet = models.resnet18(weights='IMAGENET1K_V1' if pretrained else None)
        
        self.backbone = nn.Sequential(
            resnet.conv1,
            resnet.bn1,
            resnet.relu,
            nn.MaxPool2d(kernel_size=2, stride=1, padding=1),
            resnet.layer1,
            resnet.layer2,
            resnet.layer3,
            resnet.layer4,
        )
        
        self.channels = [64, 128, 256, 512]
        
        self.downsample_layers = nn.ModuleList([
            nn.Conv2d(self.channels[0], 32, kernel_size=1),
            nn.Conv2d(self.channels[1], 64, kernel_size=1),
            nn.Conv2d(self.channels[2], 128, kernel_size=1),
            nn.Conv2d(self.channels[3], 256, kernel_size=1),
        ])
        
        total_channels = 32 + 64 + 128 + 256
        self.fusion = nn.Sequential(
            nn.Conv2d(total_channels, embed_dim, kernel_size=1),
            nn.BatchNorm2d(embed_dim),
            nn.GELU()
        )
        
        if patch_size > 1:
            self.patch_conv = nn.Conv2d(
                embed_dim, embed_dim, 
                kernel_size=patch_size, 
                stride=1,
                padding=patch_size//2
            )
        else:
            self.patch_conv = None
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        B = x.shape[0]
        
        features = []
        x_temp = x
        for i, layer in enumerate(self.backbone):
            x_temp = layer(x_temp)
            if i >= 4:
                features.append(x_temp)
        
        x1 = F.avg_pool2d(self.downsample_layers[0](features[0]), 2, 2)
        x2 = F.avg_pool2d(self.downsample_layers[1](features[1]), 2, 2)
        x3 = F.avg_pool2d(self.downsample_layers[2](features[2]), 2, 2)
        x4 = self.downsample_layers[3](features[3])
        
        target_size = x1.shape[-2:]
        
        x2 = F.interpolate(x2, size=target_size, mode='bilinear', align_corners=False)
        x3 = F.interpolate(x3, size=target_size, mode='bilinear', align_corners=False)
        x4 = F.interpolate(x4, size=target_size, mode='bilinear', align_corners=False)
        
        concatenated = torch.cat([x1, x2, x3, x4], dim=1)
        fused = self.fusion(concatenated)
        
        if self.patch_conv is not None:
            patches = self.patch_conv(fused)
            B, D, H_p, W_p = patches.shape
            patches = patches.view(B, D, -1).transpose(1, 2)
        else:
            B, D, H, W = fused.shape
            patches = fused.view(B, D, -1).transpose(1, 2)
        
        return patches

# ==================== RG-Convæ¨¡å— ====================
class RGConv(nn.Module):
    def __init__(self, in_dim, out_dim, k=9, dilation=1):
        super().__init__()
        self.in_dim = in_dim
        self.out_dim = out_dim
        self.k = k
        self.dilation = dilation
        
        self.relation_weight = nn.Sequential(
            nn.Linear(in_dim, in_dim // 2),
            nn.BatchNorm1d(in_dim // 2),
            nn.GELU(),
            nn.Linear(in_dim // 2, 1),
            nn.Sigmoid()
        )
        
        self.node_updater = nn.Sequential(
            nn.Linear(2 * in_dim, out_dim),
            nn.BatchNorm1d(out_dim),
            nn.GELU()
        )
        
        self.feature_transform = nn.Linear(in_dim, in_dim)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        B, N, D = x.shape
        device = x.device
        
        x_transformed = self.feature_transform(x)
        idx = knn(x_transformed, self.k, self.dilation)
        
        idx_base = torch.arange(0, B, device=device).view(-1, 1, 1) * N
        idx = idx + idx_base
        idx = idx.view(-1)
        
        x_reshaped = x.view(B * N, D)
        neighbors = x_reshaped[idx].view(B, N, self.k, D)
        
        center = x.unsqueeze(2).expand(B, N, self.k, D)
        edge_features = neighbors - center
        
        edge_features_flat = edge_features.view(-1, D)
        edge_weights = self.relation_weight(edge_features_flat).view(B, N, self.k, 1)
        
        aggregated = torch.sum(edge_weights * edge_features, dim=2)
        combined = torch.cat([x, aggregated], dim=2)
        combined_flat = combined.view(-1, 2 * D)
        updated = self.node_updater(combined_flat).view(B, N, self.out_dim)
        
        return updated

# ==================== RDGCNå— ====================
class RDGCNBlock(nn.Module):
    def __init__(self, in_dim, out_dim, k=9, dilation=1):
        super().__init__()
        self.in_dim = in_dim
        self.out_dim = out_dim
        self.k = k
        self.dilation = dilation
        
        self.in_trans = nn.Sequential(
            nn.Linear(in_dim, out_dim),
            nn.BatchNorm1d(out_dim),
            nn.GELU()
        )
        
        self.rg_conv = RGConv(out_dim, out_dim, k=k, dilation=dilation)
        
        self.out_trans = nn.Sequential(
            nn.Linear(out_dim, out_dim),
            nn.BatchNorm1d(out_dim),
            nn.GELU()
        )
        
        if in_dim != out_dim:
            self.residual_proj = nn.Linear(in_dim, out_dim)
        else:
            self.residual_proj = nn.Identity()
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        B, N, D = x.shape
        identity = x
        
        x_transformed = self.in_trans(x.reshape(B*N, D)).reshape(B, N, -1)
        x_conv = self.rg_conv(x_transformed)
        x_out = self.out_trans(x_conv.reshape(B*N, -1)).reshape(B, N, -1)
        
        identity = self.residual_proj(identity.reshape(B*N, D)).reshape(B, N, -1)
        x_out = x_out + identity
        
        return x_out

# ==================== ç‰¹å¾æå–å™¨ ====================
class Face2NodesFeatureExtractor(nn.Module):
    """
    é¢éƒ¨è¡¨æƒ…è¯†åˆ«æ¨¡å‹çš„ç‰¹å¾æå–å™¨
    æ”¯æŒè¾“å‡ºä»»æ„ç»´åº¦çš„ç‰¹å¾å‘é‡
    """
    def __init__(self, 
                 model_path=None,
                 embed_dim: int = 256,
                 output_dim: int = 512,
                 num_blocks: int = 4,
                 k: int = 8,
                 dilation: int = 2,
                 feature_level: str = 'global',
                 use_projection: bool = True,
                 device='cuda' if torch.cuda.is_available() else 'cpu'):
        
        super().__init__()
        
        self.device = device
        self.embed_dim = embed_dim
        self.output_dim = output_dim
        self.feature_level = feature_level
        self.use_projection = use_projection
        
        print(f"åˆå§‹åŒ–ç‰¹å¾æå–å™¨ï¼Œè®¾å¤‡: {device}")
        print(f"åŸå§‹ç‰¹å¾ç»´åº¦: {embed_dim}, è¾“å‡ºç‰¹å¾ç»´åº¦: {output_dim}")
        
        # æ„å»ºæ¨¡å‹ç»“æ„
        self.patch_embedding = MultiScalePatchEmbedding(
            in_channels=3,
            embed_dim=embed_dim,
            patch_size=1
        )
        
        self.rdgcn_blocks = nn.ModuleList()
        self.rdgcn_blocks.append(RDGCNBlock(embed_dim, embed_dim, k=k, dilation=1))
        for i in range(1, num_blocks-1):
            current_dilation = 2 ** i
            self.rdgcn_blocks.append(RDGCNBlock(embed_dim, embed_dim, k=k, dilation=current_dilation))
        self.rdgcn_blocks.append(RDGCNBlock(embed_dim, embed_dim, k=k, dilation=2**(num_blocks-1)))
        
        self.global_pool = nn.AdaptiveAvgPool1d(1)
        
        # ç‰¹å¾æŠ•å½±å±‚
        if use_projection and output_dim != embed_dim:
            self.feature_projection = nn.Sequential(
                nn.Linear(embed_dim, output_dim),
                nn.BatchNorm1d(output_dim),
                nn.GELU(),
                nn.Dropout(0.1)
            )
            print(f"æ·»åŠ ç‰¹å¾æŠ•å½±å±‚: {embed_dim} -> {output_dim}")
        else:
            self.feature_projection = nn.Identity()
        
        # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
        if model_path and os.path.exists(model_path):
            self.load_pretrained(model_path)
        else:
            print(f"è­¦å‘Š: æ¨¡å‹æ–‡ä»¶ {model_path} ä¸å­˜åœ¨ï¼Œä½¿ç”¨éšæœºåˆå§‹åŒ–")
        
        self.to(device)
        self.eval()
    
    def load_pretrained(self, model_path):
        print(f"åŠ è½½é¢„è®­ç»ƒæ¨¡å‹: {model_path}")
        checkpoint = torch.load(model_path, map_location=self.device)
        
        if isinstance(checkpoint, dict):
            if 'model_state_dict' in checkpoint:
                state_dict = checkpoint['model_state_dict']
            else:
                state_dict = checkpoint
        else:
            state_dict = checkpoint.state_dict() if hasattr(checkpoint, 'state_dict') else checkpoint
        
        keys_to_remove = [k for k in state_dict.keys() if 'head.' in k]
        for k in keys_to_remove:
            del state_dict[k]
        
        self.load_state_dict(state_dict, strict=False)
        print(f"æ¨¡å‹åŠ è½½æˆåŠŸï¼")
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = x.to(self.device)
        
        patches = self.patch_embedding(x)
        current = patches
        for block in self.rdgcn_blocks:
            current = block(current)
        
        global_features = self.global_pool(current.transpose(1, 2)).squeeze(2)
        
        if self.use_projection:
            global_features = self.feature_projection(global_features)
        
        return global_features
    
    @torch.no_grad()
    def extract_features(self, images):
        self.eval()
        
        if isinstance(images, str):
            image = self.load_image(images)
            image_tensor = image.unsqueeze(0).to(self.device)
            features = self.forward(image_tensor)
        elif isinstance(images, torch.Tensor):
            if images.dim() == 3:
                images = images.unsqueeze(0)
            features = self.forward(images.to(self.device))
        elif isinstance(images, list):
            tensors = []
            for img in images:
                if isinstance(img, str):
                    tensors.append(self.load_image(img))
                else:
                    tensors.append(img)
            batch = torch.stack(tensors).to(self.device)
            features = self.forward(batch)
        else:
            features = self.forward(images)
        
        return features.cpu()
    
    def load_image(self, image_path, size=(100, 100)):
        transform = transforms.Compose([
            transforms.Resize(size),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406],
                               std=[0.229, 0.224, 0.225])
        ])
        image = Image.open(image_path).convert('RGB')
        return transform(image)


# ==================== è§†é¢‘å¤„ç†å™¨ ====================
class VideoFaceProcessor:
    """è§†é¢‘äººè„¸ç‰¹å¾æå–å™¨"""
    
    def __init__(self, extractor, device='cuda' if torch.cuda.is_available() else 'cpu'):
        self.extractor = extractor
        self.device = device
        
        # åŠ è½½äººè„¸æ£€æµ‹å™¨
        cascade_path = cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'
        self.face_cascade = cv2.CascadeClassifier(cascade_path)
        
        if self.face_cascade.empty():
            print("âš ï¸ è­¦å‘Šï¼šäººè„¸æ£€æµ‹å™¨åŠ è½½å¤±è´¥ï¼Œå°†ä½¿ç”¨æ•´å¼ å›¾ç‰‡")
        
        print("âœ“ è§†é¢‘å¤„ç†å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def detect_faces(self, frame):
        """æ£€æµ‹è§†é¢‘å¸§ä¸­çš„äººè„¸"""
        if self.face_cascade.empty():
            h, w = frame.shape[:2]
            return [(0, 0, w, h)]
        
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        faces = self.face_cascade.detectMultiScale(
            gray, scaleFactor=1.1, minNeighbors=5, minSize=(60, 60)
        )
        return faces
    
    def preprocess_face(self, face_img):
        """é¢„å¤„ç†äººè„¸å›¾åƒ"""
        face_rgb = cv2.cvtColor(face_img, cv2.COLOR_BGR2RGB)
        pil_image = Image.fromarray(face_rgb)
        
        transform = transforms.Compose([
            transforms.Resize((100, 100)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406],
                               std=[0.229, 0.224, 0.225])
        ])
        
        return transform(pil_image).unsqueeze(0).to(self.device)
    
    def process_video(self, video_path, output_dir, sample_rate=5):
        # æ£€æŸ¥è§†é¢‘æ–‡ä»¶                        
        if not os.path.exists(video_path):
            print(f"âŒ é”™è¯¯ï¼šè§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨ - {video_path}")
            return None
    
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)
        video_name = os.path.splitext(os.path.basename(video_path))[0]
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
        # æ‰“å¼€è§†é¢‘
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            print(f"âŒ é”™è¯¯ï¼šæ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶ - {video_path}")
            return None
    
        # è·å–è§†é¢‘ä¿¡æ¯
        fps = cap.get(cv2.CAP_PROP_FPS)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    
        print(f"\nğŸ“¹ è§†é¢‘ä¿¡æ¯:")
        print(f"  æ–‡ä»¶: {video_path}")
        print(f"  æ€»å¸§æ•°: {total_frames}")
        print(f"  å¸§ç‡: {fps:.2f} fps")
        print(f"  é‡‡æ ·ç‡: æ¯{sample_rate}å¸§")
    
        # å‡†å¤‡ä¿å­˜ç»“æœ
        all_features = []
        all_timestamps = []
        all_frame_ids = []
    
        frame_count = 0
        processed_count = 0
    
        print(f"\nå¼€å§‹å¤„ç†è§†é¢‘...")
    
        while True:
            ret, frame = cap.read()
            if not ret:
                break
        
            if frame_count % sample_rate == 0:
                faces = self.detect_faces(frame)
            
                for (x, y, w, h) in faces:
                    face_roi = frame[y:y+h, x:x+w]
                
                    try:
                        input_tensor = self.preprocess_face(face_roi)
                    
                        with torch.no_grad():
                            features = self.extractor(input_tensor)
                    
                        timestamp_sec = frame_count / fps
                        all_features.append(features.cpu().numpy().squeeze())
                        all_timestamps.append(timestamp_sec)
                        all_frame_ids.append(frame_count)
                    
                        processed_count += 1
                    
                    except Exception as e:
                        print(f"  å¸§ {frame_count} ç‰¹å¾æå–å¤±è´¥: {e}")
            
                if processed_count % 10 == 0:
                    progress = frame_count / total_frames * 100
                    print(f"  è¿›åº¦: {progress:.1f}% | å·²æå– {processed_count} ä¸ªäººè„¸")
        
            frame_count += 1
    
        cap.release()
    
        print(f"\nâœ… è§†é¢‘å¤„ç†å®Œæˆï¼")
        print(f"  æ€»å¤„ç†å¸§æ•°: {frame_count}")
        print(f"  æå–ç‰¹å¾æ•°: {len(all_features)}")
     
        if len(all_features) == 0:
            print("âš ï¸ æœªæ£€æµ‹åˆ°äººè„¸")
            return None
    
        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        features_array = np.array(all_features)
    
        # ========== åªä¿å­˜.npyæ–‡ä»¶ ==========
        print(f"\nğŸ’¾ ä¿å­˜ç‰¹å¾æ–‡ä»¶...")
    
        feature_file = os.path.join(output_dir, f"{video_name}_features_{timestamp}.npy")
        np.save(feature_file, features_array)
        print(f"  âœ“ ç‰¹å¾æ–‡ä»¶: {feature_file}")
        print(f"    ç‰¹å¾çŸ©é˜µå½¢çŠ¶: {features_array.shape}")
    
        # ========== åˆ é™¤JSONå’ŒCSVä¿å­˜ä»£ç  ==========
    
        print(f"\nğŸ“Š ç‰¹å¾ç»Ÿè®¡:")
        print(f"  å‡å€¼: {features_array.mean():.6f}")
        print(f"  æ ‡å‡†å·®: {features_array.std():.6f}")
    
        return feature_file  # åªè¿”å›æ–‡ä»¶è·¯å¾„


# ==================== ä¸»ç¨‹åº ====================

def main():
    """ä¸»ç¨‹åº - çº¯è§†é¢‘å¤„ç†ï¼Œæ— éœ€ç”¨æˆ·è¾“å…¥"""
    
    print("="*60)
    print("Face2Nodes è§†é¢‘ç‰¹å¾æå–ç³»ç»Ÿ")
    print("="*60)
    print(f"æ¨¡å‹è·¯å¾„: {MODEL_PATH}")
    print(f"è§†é¢‘è·¯å¾„: {VIDEO_PATH}")
    print(f"è¾“å‡ºç›®å½•: {OUTPUT_DIR}")
    print(f"é‡‡æ ·ç‡: æ¯{SAMPLE_RATE}å¸§")
    print("="*60)
    
    # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
    if not os.path.exists(MODEL_PATH):
        print(f"âŒ é”™è¯¯ï¼šæ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨ - {MODEL_PATH}")
        return
    
    # æ£€æŸ¥è§†é¢‘æ–‡ä»¶
    if not os.path.exists(VIDEO_PATH):
        print(f"âŒ é”™è¯¯ï¼šè§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨ - {VIDEO_PATH}")
        return
    
    # åˆå§‹åŒ–è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"\nä½¿ç”¨è®¾å¤‡: {device}")
    
    # åˆå§‹åŒ–ç‰¹å¾æå–å™¨
    print("\nåˆå§‹åŒ–ç‰¹å¾æå–å™¨...")
    extractor = Face2NodesFeatureExtractor(
        model_path=MODEL_PATH,
        embed_dim=256,
        output_dim=OUTPUT_DIM,
        num_blocks=4,
        k=8,
        dilation=2,
        feature_level='global',
        use_projection=True,
        device=device
    )
    
    # åˆå§‹åŒ–è§†é¢‘å¤„ç†å™¨
    print("\nåˆå§‹åŒ–è§†é¢‘å¤„ç†å™¨...")
    video_processor = VideoFaceProcessor(extractor, device=device)
    
    # å¤„ç†è§†é¢‘
    print("\nå¼€å§‹å¤„ç†è§†é¢‘...")
    start_time = time.time()
    
    feature_file = video_processor.process_video(
        video_path=VIDEO_PATH,
        output_dir=OUTPUT_DIR,
        sample_rate=SAMPLE_RATE
    )
    
    elapsed_time = time.time() - start_time
    
    if feature_file:
        print(f"\nâœ… å¤„ç†æˆåŠŸï¼")
        print(f"  æ€»ç”¨æ—¶: {elapsed_time:.1f}ç§’")
        print(f"  è¾“å‡ºæ–‡ä»¶: {feature_file}")
    else:
        print(f"\nâŒ å¤„ç†å¤±è´¥")


if __name__ == '__main__':
    main()